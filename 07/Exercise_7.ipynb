{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joshua Campos\n",
    "## Lab Course Machine Learning\n",
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some general functions to split our data into train and test set, to calculate the accuracy of our classification model and to calculate the RMSE of our regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(x_data,y_data,train_percentage):\n",
    "    row_selection = np.random.rand(len(x_data)) < train_percentage\n",
    "    x_train = x_data[row_selection]\n",
    "    y_train = pd.DataFrame(y_data[row_selection])\n",
    "    x_test = x_data[~row_selection]\n",
    "    y_test = pd.DataFrame(y_data[~row_selection])\n",
    "    return (x_train,y_train,x_test,y_test)\n",
    "\n",
    "def calculate_accuracy(y_test,y_predicted):\n",
    "    correct_predictions = 0\n",
    "    y_test_arr = np.array(y_test)\n",
    "    for i in range(len(y_test)):\n",
    "        if y_predicted[i] == y_test_arr[i]:\n",
    "            correct_predictions += 1\n",
    "    accuracy = correct_predictions/len(y_test)\n",
    "    num_samples = len(y_test)\n",
    "    wrong_predictions = num_samples-correct_predictions\n",
    "    return (accuracy,correct_predictions,wrong_predictions,num_samples)\n",
    "\n",
    "def calculate_rmse(y_test,y_predicted):\n",
    "    y_test_arr = np.array(y_test)\n",
    "    return np.sqrt(np.square(np.subtract(y_test_arr,y_predicted)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN Functions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the functions we are going to use for our KNN algorithm. These functions are used to calculate the euclidian distance between data points, get the neighbors according to the given 'k', get the neighbors' index and class, get the majority voting for our classification model, and finally the KNN prediction function for classification and regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_euclidian_distance(X_train,X_test_instance):\n",
    "    train_samples = X_train.shape[0]\n",
    "    total_distance = []\n",
    "    for i in range(train_samples):\n",
    "        distance = 0\n",
    "        features = X_train.columns\n",
    "        for feature in features:\n",
    "            distance += np.sqrt(np.square(X_train[feature].iloc[i] - X_test_instance[feature]))\n",
    "        total_distance.append(distance)\n",
    "    return np.array(total_distance)\n",
    "\n",
    "def get_neighbors(k,distances):\n",
    "    dist_arr = np.copy(distances)\n",
    "    k_neighbors = []\n",
    "    for i in range(k):\n",
    "        min_index = np.argmin(dist_arr)\n",
    "        min_dist = np.min(dist_arr)\n",
    "        k_neighbors.append(min_dist)\n",
    "        dist_arr = np.delete(dist_arr,min_index)\n",
    "    return k_neighbors\n",
    "\n",
    "def get_neighbors_index(distances,neighbors):\n",
    "    indices = []\n",
    "    for value in neighbors:\n",
    "        index = np.where(distances == value)\n",
    "        indices.append(index[0][0])\n",
    "    return indices\n",
    "\n",
    "def get_neighbors_class(y_train,indices):\n",
    "    classes = []\n",
    "    y_classes = np.array(y_train)\n",
    "    for index in indices:\n",
    "        classes.append(y_classes[index])\n",
    "    return classes\n",
    "\n",
    "def get_majority_voting(classes):\n",
    "    uniques = np.unique(classes)\n",
    "    class_count = []\n",
    "    for element in uniques:\n",
    "        class_count.append(classes.count(element))\n",
    "    max_class = np.max(class_count)\n",
    "    max_index = np.where(class_count == max_class)[0][0]\n",
    "    pred_class = uniques[max_index]\n",
    "    return pred_class\n",
    "\n",
    "def predict_knn_class(X_train,y_train,X_test,y_test,k):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        test_sample = X_test.iloc[i,:]\n",
    "        total_distance = calculate_euclidian_distance(X_train,test_sample)\n",
    "        neighbors = get_neighbors(k,total_distance)\n",
    "        indices = get_neighbors_index(total_distance,neighbors)\n",
    "        classes = get_neighbors_class(y_train,indices)\n",
    "        pred_class = get_majority_voting(classes)\n",
    "        predictions.append(pred_class)\n",
    "    return predictions\n",
    "\n",
    "def predict_knn_reg(X_train,y_train,X_test,y_test,k):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        test_sample = X_test.iloc[i,:]\n",
    "        total_distance = calculate_euclidian_distance(X_train,test_sample)\n",
    "        neighbors = get_neighbors(k,total_distance)\n",
    "        indices = get_neighbors_index(total_distance,neighbors)\n",
    "        classes = get_neighbors_class(y_train,indices)\n",
    "        class_mean = round(np.mean(classes))\n",
    "        predictions.append(class_mean)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation Functions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the functions for our cross validation, which we will be using to determine the optimal value of 'k'. We create a function to create folds/groups from our data to use as training and validations sets, and a function to peform the cross validation across the previously created folds given the 'k' values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(X_train,y_train,num_folds): \n",
    "    folds = [] \n",
    "    data = np.hstack((X_train, y_train))\n",
    "    np.random.shuffle(data) \n",
    "    fold_size = data.shape[0] // num_folds\n",
    "    for i in range(num_folds): \n",
    "        fold = data[i * fold_size:(i + 1)*fold_size, :]\n",
    "        X_fold = fold[:, :-1] \n",
    "        y_fold = fold[:, -1].reshape((-1, 1))\n",
    "        folds.append((X_fold, y_fold))  \n",
    "    return folds\n",
    "\n",
    "def k_fold_cross_validation(folds,k_array,problem_type,column_names):\n",
    "    parameters_df = pd.DataFrame(columns=['K Neighbors','Avg. Measure'])\n",
    "    for k in k_array:\n",
    "        fold_measures = []\n",
    "        for i in range(len(folds)-1):\n",
    "            x_test_f = pd.DataFrame(folds[i][0],columns=column_names[:-1])\n",
    "            y_test_f = pd.DataFrame(folds[i][1],columns=[column_names[-1]])\n",
    "            x_train_f = pd.DataFrame(columns=column_names[:-1])\n",
    "            y_train_f = pd.DataFrame(columns=[column_names[-1]])\n",
    "            \n",
    "            for n in [x for x in range(len(folds)) if x != i]:\n",
    "                x_train_f = x_train_f.append(pd.DataFrame(folds[n][0],columns=column_names[:-1]))\n",
    "                y_train_f = y_train_f.append(pd.DataFrame(folds[n][1],columns=[column_names[-1]]))\n",
    "            if problem_type == 'classification':\n",
    "                predictions = predict_knn_class(x_train_f,y_train_f,x_test_f,y_test_f,k)\n",
    "                y_test_arr = np.array(y_test_f)\n",
    "                accuracy = calculate_accuracy(y_test_arr,predictions)[0]\n",
    "                fold_measures.append(accuracy)\n",
    "            elif problem_type == 'regression':\n",
    "                predictions = predict_knn_reg(x_train_f,y_train_f,x_test_f,y_test_f,k)\n",
    "                y_test_arr = np.array(y_test_f)\n",
    "                rmse = calculate_rmse(y_test_arr,predictions)\n",
    "                fold_measures.append(rmse)\n",
    "            else:\n",
    "                raise Exception('Invalid problem type: choose \"classification\" or \"regression\"')\n",
    "        \n",
    "        measure_mean = np.mean(fold_measures)\n",
    "        parameters_df = parameters_df.append({'K Neighbors':k,'Avg. Measure':measure_mean},\n",
    "                                             ignore_index=True)\n",
    "    \n",
    "    if problem_type == 'classification':\n",
    "        optimal_row = parameters_df[parameters_df['Avg. Measure'] == np.max(\n",
    "            parameters_df['Avg. Measure'])]\n",
    "    elif problem_type == 'regression':\n",
    "        optimal_row = parameters_df[parameters_df['Avg. Measure'] == np.min(\n",
    "            parameters_df['Avg. Measure'])]\n",
    "    \n",
    "    optimal_k = optimal_row['K Neighbors'].values[0]\n",
    "    best_measure = optimal_row['Avg. Measure'].values[0]\n",
    "    return (parameters_df,optimal_k,best_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read our iris data file and name the columns accordingly. We then print the head to understand how our data looks like. After this, we can separate our data into independent and dependent variables, which will then be separated into our training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length  sepal width  petal length  petal width        class\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "features = ['sepal length','sepal width','petal length','petal width','class']\n",
    "iris_data = pd.read_csv('iris.data',names=features)\n",
    "print(iris_data.head())\n",
    "\n",
    "X = iris_data.drop('class',axis=1)\n",
    "y = iris_data['class']\n",
    "\n",
    "train_percentage = 0.7\n",
    "X_train,y_train,X_test,y_test = split_train_test(X,y,train_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our KNN Classification algorithm to predict the class of our test data, using a 'k' of 5. After this, we print the accuracy of our model, as well as the correct predictions, wrong predictions and total number of samples predicted. We decided to use the accuracy as quality criterion for our model, because it gives us a general idea of how our model is performing. We want to know how many predictions were correct out of the whole test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classification: Iris Dataset\n",
      "K: 5\n",
      "Accuracy: 0.9\n",
      "Correct Predictions: 36      \n",
      "Incorrect Predictions: 4\n",
      "Total Samples: 40\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "predictions = predict_knn_class(X_train,y_train,X_test,y_test,k)\n",
    "accuracy,correct_predictions,wrong_predictions,num_samples = calculate_accuracy(y_test,predictions)\n",
    "\n",
    "print('KNN Classification: Iris Dataset\\nK: {}\\nAccuracy: {}\\nCorrect Predictions: {}\\\n",
    "      \\nIncorrect Predictions: {}\\nTotal Samples: {}'.format(k,accuracy,correct_predictions,\n",
    "      wrong_predictions,num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the Optimal Value for K in KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our train data into four folds, so we can perform the cross validation with different values for 'k', which were defined to be all the odd numbers from one to 15. After the cross validation, we print the average accuracy for each 'k' used, and also the optimal 'k' with its accuracy measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   K Neighbors  Avg. Measure\n",
      "0          1.0      0.925926\n",
      "1          3.0      0.962963\n",
      "2          5.0      0.987654\n",
      "3          7.0      0.987654\n",
      "4          9.0      0.975309\n",
      "5         11.0      0.975309\n",
      "6         13.0      0.962963\n",
      "7         15.0      0.975309\n",
      "\n",
      "Optimal K: 5.0\n",
      "Best Measure: 0.9876543209876543\n"
     ]
    }
   ],
   "source": [
    "folds = create_folds(X_train,y_train,4)\n",
    "k_list = [1,3,5,7,9,11,13,15]\n",
    "problem_type = 'classification'\n",
    "column_names = iris_data.columns\n",
    "\n",
    "parameters_df,optimal_k,best_measure = k_fold_cross_validation(folds,k_list,problem_type,column_names)\n",
    "\n",
    "print(parameters_df)\n",
    "print('\\nOptimal K: {}\\nBest Measure: {}'.format(optimal_k,best_measure))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run again the KNN Classification algorithm, but this time we use the optimal 'k' value given in the cross validation. We then print the model accuracy, correct predictions, wrong predictions and the total test samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classification: Iris Dataset\n",
      "K: 5\n",
      "Accuracy: 0.9\n",
      "Correct Predictions: 36      \n",
      "Incorrect Predictions: 4\n",
      "Total Samples: 40\n"
     ]
    }
   ],
   "source": [
    "k = int(optimal_k)\n",
    "predictions = predict_knn_class(X_train,y_train,X_test,y_test,k)\n",
    "accuracy,correct_predictions,wrong_predictions,num_samples = calculate_accuracy(y_test,predictions)\n",
    "\n",
    "print('KNN Classification: Iris Dataset\\nK: {}\\nAccuracy: {}\\nCorrect Predictions: {}\\\n",
    "      \\nIncorrect Predictions: {}\\nTotal Samples: {}'.format(k,accuracy,correct_predictions,\n",
    "      wrong_predictions,num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** An observation for this case is that the optimal 'k' value given here is not the definitive best value. The algorithm was run several times and it didn't always give the same optimal 'k'. This value depends a lot on the dataset and how it is divided for the train and test set. It is easier to predict the classes if the values are centered between data points of the same class, but if a datapoint is close to the border between another class, it might miss the prediction. We can also see that after doing the cross-validation, we best performing 'k' value was 5, as we chose previously, but this wasn't always the case when we ran the algorithm several times before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wine Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the wine data file and split it into our independent and dependent variables, which we then split into our training and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = pd.read_csv('winequality-red.csv',sep=';')\n",
    "\n",
    "X = wine_data.drop('quality',axis=1)\n",
    "y = wine_data['quality']\n",
    "\n",
    "train_percentage = 0.7\n",
    "X_train,y_train,X_test,y_test = split_train_test(X,y,train_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our KNN Regression algorithm to predict the class of our test data, using a 'k' of 40, which we defined by getting the squared root of the size of the dataset. After this, we print the accuracy of our model, as well as the correct predictions, wrong predictions and total number of samples predicted. We decided to use the RMSE and the accuracy as quality criterion for our model, because the RMSE gives us an idea of the error rate that we had for our model and the accuracy gives us a general idea of how our model is performing, by calculating how many predictions were correct out of the whole test set. Usually, accuracy is not used to calculate quality in regression models, but because the class we are predicting are whole numbers and our model's predictions are also whole numbers, we can use this method as well to see if our model manages to predict correctly each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Regression: Wine Dataset\n",
      "K: 40\n",
      "RMSE: 0.9611399681493781\n",
      "Accuracy: 0.5221238938053098\n",
      "Correct Predictions: 236      \n",
      "Incorrect Predictions: 216\n",
      "Total Samples: 452\n"
     ]
    }
   ],
   "source": [
    "k = int(round(np.sqrt(len(wine_data))))\n",
    "predictions = predict_knn_reg(X_train,y_train,X_test,y_test,k)\n",
    "accuracy,correct_predictions,wrong_predictions,num_samples = calculate_accuracy(y_test,predictions)\n",
    "rmse = calculate_rmse(y_test,predictions)\n",
    "\n",
    "print('KNN Regression: Wine Dataset\\nK: {}\\nRMSE: {}\\nAccuracy: {}\\nCorrect Predictions: {}\\\n",
    "      \\nIncorrect Predictions: {}\\nTotal Samples: {}'.format(k,rmse,accuracy,correct_predictions,\n",
    "      wrong_predictions,num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the Optimal Value for K in KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our train data into four folds, so we can perform the cross validation with different values for 'k', which were defined to be every 10 numbers from 5 to 75. For this model we choose to use higher values for 'k', because the dataset is larger, so we want to see how the model performs with higher values. After the cross validation, we print the average accuracy for each 'k' used, and also the optimal 'k' with its accuracy measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   K Neighbors  Avg. Measure\n",
      "0          5.0      0.996256\n",
      "1         15.0      0.961934\n",
      "2         25.0      0.949158\n",
      "3         35.0      0.945225\n",
      "4         45.0      0.942646\n",
      "5         55.0      0.939697\n",
      "6         65.0      0.934872\n",
      "7         75.0      0.931723\n",
      "\n",
      "Optimal K: 75.0\n",
      "Best RMSE: 0.9317231087823933\n"
     ]
    }
   ],
   "source": [
    "folds = create_folds(X_train,y_train,4)\n",
    "k_list = [5,15,25,35,45,55,65,75]\n",
    "problem_type = 'regression'\n",
    "column_names = wine_data.columns\n",
    "\n",
    "parameters_df,optimal_k,best_measure = k_fold_cross_validation(folds,k_list,problem_type,column_names)\n",
    "\n",
    "print(parameters_df)\n",
    "print('\\nOptimal K: {}\\nBest RMSE: {}'.format(optimal_k,best_measure))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run again the KNN Regression algorithm, but this time we use the optimal 'k' value given in the cross validation. We then print the model accuracy, correct predictions, wrong predictions and the total test samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Regression: Wine Dataset\n",
      "K: 75\n",
      "RMSE: 0.9514663409598269\n",
      "Accuracy: 0.5088495575221239\n",
      "Correct Predictions: 230      \n",
      "Incorrect Predictions: 222\n",
      "Total Samples: 452\n"
     ]
    }
   ],
   "source": [
    "k = int(optimal_k)\n",
    "predictions = predict_knn_reg(X_train,y_train,X_test,y_test,k)\n",
    "accuracy,correct_predictions,wrong_predictions,num_samples = calculate_accuracy(y_test,predictions)\n",
    "rmse = calculate_rmse(y_test,predictions)\n",
    "\n",
    "print('KNN Regression: Wine Dataset\\nK: {}\\nRMSE: {}\\nAccuracy: {}\\nCorrect Predictions: {}\\\n",
    "      \\nIncorrect Predictions: {}\\nTotal Samples: {}'.format(k,rmse,accuracy,correct_predictions,\n",
    "      wrong_predictions,num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** We can see that something really interesting is happening in this case. When we used the optimal 'k' we got a lower RMSE, which is better for our problem, which is a KNN Regression, but we got a lower accuracy, which is worse, than when we used 40 as fixed 'k' value. If we think about this, we can understand that we missed more exact class predictions with the optimal 'k', but the error for each missed class was lower than with a fixed 'k' value, as we can tell from the lower RMSE. We can also see from the cross-validations, that the higher the 'k', the better the model performed, which might be because we get a more accurate average from its neighbors. In conclusion, we can say that our model performed better, because we are using RMSE as the main quality criteria for our regression. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
